
from flask import Flask, request, jsonify
from flask_cors import CORS
from PIL import Image
import torch
from torch import nn
from torchvision import transforms
from transformers import SwinModel, BlipProcessor, BlipForConditionalGeneration
import torch.nn.functional as F
import requests
import json
import os

# ============= Flaskåº”ç”¨åˆå§‹åŒ– =============
app = Flask(__name__)
CORS(app)  # å¯ç”¨è·¨åŸŸèµ„æºå…±äº«

# æ£€æµ‹å¹¶è®¾ç½®è®¡ç®—è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ============= å›¾åƒé¢„å¤„ç† =============
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])


# ============= 1. MOEæ¨¡å—ï¼ˆå¤šæ¨¡æ€åŒè¾“å…¥ç‰ˆæœ¬ï¼‰=============
class MOE(nn.Module):
    """
    æ¨¡æ€ä¸“å®¶æ··åˆ(Mixture of Experts)æ¨¡å—

    âœ… å…³é”®ç‰¹æ€§ï¼šæ¥æ”¶è§†è§‰å’Œæ–‡æœ¬åŒè¾“å…¥ï¼Œè¿›è¡Œå¤šæ¨¡æ€èåˆ

    å‚æ•°:
        num_experts (int): ä¸“å®¶ç½‘ç»œçš„æ•°é‡
        visual_input_size (int): è§†è§‰ç‰¹å¾çš„ç»´åº¦ï¼ˆSwinè¾“å‡ºï¼š768ï¼‰
        text_input_size (int): æ–‡æœ¬ç‰¹å¾çš„ç»´åº¦ï¼ˆBLIPè¾“å‡ºï¼š768ï¼‰
        expert_output_size (int): æ¯ä¸ªä¸“å®¶ç½‘ç»œè¾“å‡ºçš„ç»´åº¦ï¼ˆ512ï¼‰
        dropout_rate (float): Dropoutæ¯”ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    """
    def __init__(self, num_experts, visual_input_size, text_input_size, expert_output_size, dropout_rate=0.1):
        super(MOE, self).__init__()
        self.num_experts = num_experts

        # âœ… ç‰¹å¾æŠ•å½±å±‚ï¼Œå°†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾æŠ•å½±åˆ°ç›¸åŒçš„ç»´åº¦
        self.visual_projection = nn.Linear(visual_input_size, expert_output_size)
        self.text_projection = nn.Linear(text_input_size, expert_output_size)

        # âœ… è®¡ç®—é—¨æ§ç½‘ç»œè¾“å…¥ç»´åº¦ï¼ˆè§†è§‰ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾çš„æ‹¼æ¥ï¼‰
        gate_input_size = expert_output_size * 2  # 512 * 2 = 1024

        # âœ… åˆ›å»ºä¸“å®¶ç½‘ç»œï¼Œæ¯ä¸ªä¸“å®¶ç”±ä¸¤ä¸ªçº¿æ€§å±‚ã€æ¿€æ´»å‡½æ•°ã€Dropoutå’ŒLayerNormç»„æˆ
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(gate_input_size, expert_output_size),  # ç¬¬ä¸€ä¸ªçº¿æ€§å±‚
                nn.GELU(),  # GELUæ¿€æ´»å‡½æ•°
                nn.Dropout(dropout_rate),  # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
                nn.Linear(expert_output_size, expert_output_size),  # ç¬¬äºŒä¸ªçº¿æ€§å±‚
                nn.LayerNorm(expert_output_size)  # å±‚å½’ä¸€åŒ–
            ) for _ in range(num_experts)
        ])

        # âœ… é—¨æ§ç½‘ç»œï¼Œç”¨äºå†³å®šæ¯ä¸ªä¸“å®¶çš„æƒé‡
        self.gate = nn.Linear(gate_input_size, num_experts)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, visual_features, text_features):
        """
        âœ… å‰å‘ä¼ æ’­å‡½æ•°ï¼ˆåŒè¾“å…¥ï¼‰

        å‚æ•°:
            visual_features (torch.Tensor): è§†è§‰ç‰¹å¾å¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, 768)
            text_features (torch.Tensor): æ–‡æœ¬ç‰¹å¾å¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, 768)

        è¿”å›:
            torch.Tensor: èåˆåçš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(batch_size, 512)
        """
        # âœ… æŠ•å½±ç‰¹å¾åˆ°ç›¸åŒç»´åº¦
        projected_visual = self.visual_projection(visual_features)  # (batch, 512)
        projected_text = self.text_projection(text_features)        # (batch, 512)

        # âœ… æ‹¼æ¥ç‰¹å¾ä½œä¸ºMOEçš„è¾“å…¥
        combined_features = torch.cat([projected_visual, projected_text], dim=1)  # (batch, 1024)

        # è®¡ç®—é—¨æ§æƒé‡
        gate_logits = self.gate(combined_features)  # (batch, 6)
        gate_probabilities = F.softmax(gate_logits, dim=1)  # ä½¿ç”¨softmaxå½’ä¸€åŒ–æƒé‡
        gate_probabilities = self.dropout(gate_probabilities)  # å¢åŠ éšæœºæ€§

        # é€šè¿‡æ¯ä¸ªä¸“å®¶ç½‘ç»œå¤„ç†ç‰¹å¾
        expert_outputs = []
        for i in range(self.num_experts):
            expert_outputs.append(self.experts[i](combined_features))  # (batch, 512)
        expert_outputs = torch.stack(expert_outputs, dim=1)  # (batch, 6, 512)

        # Gating - ä½¿ç”¨çˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šè®¡ç®—åŠ æƒå’Œ
        gated_outputs = torch.einsum("beo,be->bo", expert_outputs, gate_probabilities)  # (batch, 512)
        return gated_outputs


# ============= 2. æ–‡æœ¬ç¼–ç å™¨æ¨¡å— =============
class TextEncoder(nn.Module):
    """
    æ–‡æœ¬ç¼–ç å™¨æ¨¡å—ï¼Œç”¨äºå¤„ç†BLIPç”Ÿæˆçš„å›¾åƒæè¿°

    å‚æ•°:
        hidden_size (int): éšè—å±‚å¤§å°ï¼ˆ768ï¼‰
        output_size (int): è¾“å‡ºç‰¹å¾ç»´åº¦ï¼ˆ512ï¼‰
        dropout_rate (float): Dropoutæ¯”ç‡
    """
    def __init__(self, hidden_size=768, output_size=512, dropout_rate=0.1):
        super(TextEncoder, self).__init__()
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.act1 = nn.GELU()
        self.norm1 = nn.LayerNorm(hidden_size)
        self.drop1 = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.act2 = nn.GELU()
        self.norm2 = nn.LayerNorm(output_size)
        self.drop2 = nn.Dropout(dropout_rate)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°

        å‚æ•°:
            x (torch.Tensor): è¾“å…¥æ–‡æœ¬ç‰¹å¾

        è¿”å›:
            torch.Tensor: å¤„ç†åçš„æ–‡æœ¬ç‰¹å¾
        """
        x = self.fc1(x)
        x = self.act1(x)
        x = self.norm1(x)
        x = self.drop1(x)
        x = self.fc2(x)
        x = self.act2(x)
        x = self.norm2(x)
        x = self.drop2(x)
        return x


# ============= 3. å¤šæ¨¡æ€ç¾å­¦è´¨é‡è¯„ä¼°æ¨¡å‹ =============
class MultimodalAestheticModel(nn.Module):
    """
    å¤šæ¨¡æ€ç¾å­¦è´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾

    å‚æ•°:
        num_experts (int): MOEæ¨¡å—ä¸­çš„ä¸“å®¶æ•°é‡ï¼Œé»˜è®¤ä¸º6
        expert_output_size (int): æ¯ä¸ªä¸“å®¶çš„è¾“å‡ºç»´åº¦ï¼Œé»˜è®¤ä¸º512
        dropout_rate (float): Dropoutæ¯”ç‡ï¼Œé»˜è®¤ä¸º0.1
    """
    def __init__(self, num_experts=6, expert_output_size=512, dropout_rate=0.1):
        super(MultimodalAestheticModel, self).__init__()

        # âœ… è§†è§‰ç¼–ç å™¨ - ä½¿ç”¨é¢„è®­ç»ƒçš„Swin Transformer
        self.swin = SwinModel.from_pretrained("microsoft/swin-tiny-patch4-window7-224")
        self.visual_feature_dim = self.swin.config.hidden_size  # 768

        # âœ… æ–‡æœ¬ç¼–ç å™¨ - ä½¿ç”¨BLIPæ¨¡å‹
        self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        self.text_feature_dim = self.blip_model.config.text_config.hidden_size  # 768

        # âœ… æ–‡æœ¬ç‰¹å¾å¤„ç†å™¨
        self.text_encoder = TextEncoder(
            hidden_size=self.text_feature_dim,
            output_size=expert_output_size,
            dropout_rate=dropout_rate
        )

        # å†»ç»“éƒ¨åˆ†Swin Transformerå±‚å’ŒBLIPæ¨¡å‹
        for name, param in self.swin.named_parameters():
            if 'layer.0' in name or 'layer.1' in name:  # å†»ç»“å‰ä¸¤å±‚
                param.requires_grad = False

        for param in self.blip_model.parameters():
            param.requires_grad = False  # å®Œå…¨å†»ç»“BLIPæ¨¡å‹

        # âœ… å¤šæ¨¡æ€ç‰¹å¾èåˆMOEé—¨æ§ç½‘ç»œï¼ˆåŒè¾“å…¥ï¼‰
        self.moe_gating_network = MOE(
            num_experts=num_experts,
            visual_input_size=self.visual_feature_dim,  # 768
            text_input_size=self.text_feature_dim,       # 768
            expert_output_size=expert_output_size,       # 512
            dropout_rate=dropout_rate
        )

        # âœ… ç‰¹å¾èåˆå±‚
        self.fusion_layer = nn.Linear(expert_output_size, expert_output_size)
        self.fusion_act = nn.GELU()
        self.fusion_norm = nn.LayerNorm(expert_output_size)
        self.fusion_drop = nn.Dropout(dropout_rate)

        # âœ… è¯„åˆ†é¢„æµ‹å±‚
        self.fc1 = nn.Linear(expert_output_size, 256)
        self.act1 = nn.GELU()
        self.drop1 = nn.Dropout(dropout_rate)
        self.norm1 = nn.LayerNorm(256)

        self.fc2 = nn.Linear(256, 64)
        self.act2 = nn.GELU()
        self.drop2 = nn.Dropout(dropout_rate)
        self.norm2 = nn.LayerNorm(64)

        # å›å½’å™¨ï¼Œè¾“å‡ºç¾å­¦è¯„åˆ†
        self.regressor = nn.Linear(64, 1)

        # å­˜å‚¨ç”Ÿæˆçš„å›¾åƒæè¿°ï¼ˆå¯é€‰ï¼‰
        self.generated_captions = {}

    def extract_text_features(self, images, batch_indices=None):
        """
        âœ… æå–å›¾åƒçš„æ–‡æœ¬æè¿°ç‰¹å¾

        å‚æ•°:
            images (torch.Tensor): è¾“å…¥å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, 3, 224, 224)
            batch_indices (list): æ‰¹æ¬¡ç´¢å¼•ï¼Œç”¨äºå­˜å‚¨ç”Ÿæˆçš„æè¿°ï¼ˆå¯é€‰ï¼‰

        è¿”å›:
            tuple: (æ–‡æœ¬ç‰¹å¾å¼ é‡(batch_size, 768), å›¾åƒæè¿°åˆ—è¡¨)
        """
        batch_size = images.size(0)
        text_features = torch.zeros(batch_size, self.text_feature_dim).to(images.device)
        image_captions = ["" for _ in range(batch_size)]

        # åå½’ä¸€åŒ–å›¾åƒï¼ˆä»ImageNetæ ‡å‡†æ¢å¤åˆ°[0, 1]ï¼‰
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(images.device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(images.device)
        unnormalized_images = images * std + mean
        unnormalized_images = unnormalized_images.clamp(0, 1)

        for i in range(batch_size):
            # è½¬æ¢ä¸ºPILå›¾åƒ
            img = transforms.ToPILImage()(unnormalized_images[i].cpu())

            # âœ… ä½¿ç”¨BLIPç”Ÿæˆæè¿°å¹¶æå–ç‰¹å¾
            inputs = self.blip_processor(img, return_tensors="pt").to(images.device)
            with torch.no_grad():
                # ç”Ÿæˆå›¾åƒæè¿°
                outputs = self.blip_model.generate(**inputs, max_length=50)
                caption = self.blip_processor.decode(outputs[0], skip_special_tokens=True)
                image_captions[i] = caption

                # âœ… å…³é”®ï¼šæå–æ–‡æœ¬ç‰¹å¾
                text_inputs = self.blip_processor.tokenizer(caption, return_tensors="pt").to(images.device)
                text_outputs = self.blip_model.text_encoder(**text_inputs, output_hidden_states=True)
                # ä½¿ç”¨æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡å€¼ä½œä¸ºæ–‡æœ¬ç‰¹å¾
                text_features[i] = text_outputs.hidden_states[-1].mean(dim=1).squeeze(0)

                # å¦‚æœæä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œå­˜å‚¨ç”Ÿæˆçš„æè¿°
                if batch_indices is not None:
                    self.generated_captions[batch_indices[i]] = caption

        return text_features, image_captions

    def forward(self, images, batch_indices=None):
        """
        âœ… å‰å‘ä¼ æ’­å‡½æ•°ï¼ˆå¤šæ¨¡æ€èåˆï¼‰

        å‚æ•°:
            images (torch.Tensor): è¾“å…¥å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, 3, 224, 224)
            batch_indices (list): æ‰¹æ¬¡ç´¢å¼•ï¼Œç”¨äºå­˜å‚¨ç”Ÿæˆçš„æè¿°ï¼ˆå¯é€‰ï¼‰

        è¿”å›:
            torch.Tensor: é¢„æµ‹çš„ç¾å­¦è¯„åˆ†ï¼Œå½¢çŠ¶ä¸º(batch_size, 1)
        """
        # 1. âœ… æå–è§†è§‰ç‰¹å¾ - è§†è§‰ç‰¹å¾æå–è·¯å¾„
        visual_outputs = self.swin(images)
        visual_features = visual_outputs.pooler_output  # (batch_size, 768)

        # 2. âœ… æå–æ–‡æœ¬ç‰¹å¾ - æ–‡æœ¬ç‰¹å¾æå–è·¯å¾„
        text_features, image_captions = self.extract_text_features(images, batch_indices)  # (batch_size, 768)

        # 3. âœ… å¤šæ¨¡æ€ç‰¹å¾èåˆ - MOEé—¨æ§ç½‘ç»œï¼ˆåŒè¾“å…¥ï¼ï¼‰
        # å°†è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾åŒæ—¶è¾“å…¥åˆ°MOEé—¨æ§ç½‘ç»œ
        fused_features = self.moe_gating_network(visual_features, text_features)  # (batch_size, 512)

        # 4. âœ… ç‰¹å¾èåˆåå¤„ç†
        fused_features = self.fusion_layer(fused_features)  # (batch_size, 512)
        fused_features = self.fusion_act(fused_features)
        fused_features = self.fusion_norm(fused_features)
        fused_features = self.fusion_drop(fused_features)

        # 5. âœ… é¢„æµ‹è¯„åˆ† - ç¾å­¦è¯„åˆ†é¢„æµ‹
        x = self.fc1(fused_features)
        x = self.act1(x)
        x = self.drop1(x)
        x = self.norm1(x)

        x = self.fc2(x)
        x = self.act2(x)
        x = self.drop2(x)
        x = self.norm2(x)

        score = self.regressor(x)
        return score


# ============= åŠ è½½æ¨¡å‹ =============
print("\n" + "="*70)
print("æ­£åœ¨åˆå§‹åŒ–å¤šæ¨¡æ€ç¾å­¦è¯„ä¼°æ¨¡å‹...")
print("="*70)

aesthetic_model = MultimodalAestheticModel(
    num_experts=6,
    expert_output_size=512,
    dropout_rate=0.1
).to(device)

# âœ… åŠ è½½å¤šæ¨¡æ€è®­ç»ƒæƒé‡
# æ³¨æ„ï¼šå¿…é¡»ä½¿ç”¨multimodal_train.pyè®­ç»ƒçš„æƒé‡ï¼Œä¸èƒ½ä½¿ç”¨å•æ¨¡æ€çš„version4_model.pth
model_save_path = os.path.join(os.path.dirname(__file__), '..', 'multimodal_aesthetic_model.pth')

if os.path.exists(model_save_path):
    try:
        loaded_model = torch.load(model_save_path, map_location=device, weights_only=False)

        if isinstance(loaded_model, dict):
            aesthetic_model.load_state_dict(loaded_model)
        else:
            aesthetic_model.load_state_dict(loaded_model.state_dict())

        print(f"âœ… æˆåŠŸåŠ è½½å¤šæ¨¡æ€æ¨¡å‹æƒé‡: {model_save_path}")
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿æƒé‡æ–‡ä»¶æ˜¯ä½¿ç”¨multimodal_train.pyè®­ç»ƒçš„å¤šæ¨¡æ€æ¨¡å‹")
else:
    print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶")
    print(f"   é¢„æœŸè·¯å¾„: {model_save_path}")
    print("   è¯·ç¡®ä¿ä½¿ç”¨ multimodal_train.py è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜æƒé‡")
    print("   æˆ–è€…è°ƒæ•´ model_save_path æŒ‡å‘æ­£ç¡®çš„æƒé‡æ–‡ä»¶")

aesthetic_model.eval()

print("âœ… æ¨¡å‹å·²è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
print("="*70)


# ============= APIç›¸å…³é…ç½® =============
API_KEY = "sk-4245c0d9f27f1fd3c51c86b03b6cfe12"


def generate_description_api(aesthetic_score, image_caption, api_key, prompt="è¿™å¼ å›¾ç‰‡"):
    """
    ä½¿ç”¨å¤–éƒ¨APIç”Ÿæˆè¯¦ç»†ç¾å­¦æè¿°

    å‚æ•°:
        aesthetic_score: ç¾å­¦è¯„åˆ†
        image_caption: BLIPç”Ÿæˆçš„å›¾åƒæè¿°
        api_key: APIå¯†é’¥
        prompt: æç¤ºè¯å‰ç¼€

    è¿”å›:
        str: ç”Ÿæˆçš„è¯¦ç»†æè¿°
    """
    url = "https://aibotpro.cn/v1/chat/completions"
    payload = {
        "model": "chatgpt-4o-latest",
        "messages": [
            {
                "role": "user",
                "content": f"{prompt}çš„ç¾å­¦è¯„åˆ†æ˜¯ {aesthetic_score:.2f}ã€‚æˆ‘å¯¹è¿™å¼ å›¾ç‰‡çš„æ–‡æœ¬æè¿°æ˜¯ï¼š'{image_caption}'ã€‚è¯·åŸºäºè¿™ä¸ªç¾å­¦è¯„åˆ†å’Œæ–‡æœ¬æè¿°ï¼Œæ›´è¯¦ç»†åœ°æè¿°è¿™å¼ å›¾ç‰‡ï¼Œçªå‡ºå®ƒçš„è§†è§‰å…ƒç´ å’Œæ•´ä½“ç¾æ„Ÿã€‚"
            }
        ],
        "stream": False
    }
    headers = {
        "Authorization": f"Bearer {api_key}",
        "User-Agent": "Apifox/1.0.0 (https://apifox.com)",
        "Content-Type": "application/json",
        "Accept": "*/*",
        "Host": "aibotpro.cn",
        "Connection": "keep-alive"
    }

    try:
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        response_json = response.json()
        if 'choices' in response_json and response_json['choices']:
            return response_json['choices'][0]['message']['content']
        else:
            return "Error: API æ²¡æœ‰ç”Ÿæˆæ–‡æœ¬ã€‚"
    except requests.exceptions.RequestException as e:
        return f"API è¯·æ±‚é”™è¯¯: {e}"
    except json.JSONDecodeError:
        return "é”™è¯¯: æ— æ³•è§£æ API å“åº”ä¸º JSONã€‚"


# ============= Flaskè·¯ç”± =============

@app.route('/test', methods=['GET'])
def test():
    """æµ‹è¯•è·¯ç”± - éªŒè¯æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"""
    return jsonify({
        'message': 'âœ… å¤šæ¨¡æ€ç¾å­¦è¯„ä»·ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼',
        'model': 'MultimodalAestheticModel',
        'architecture': {
            'visual_encoder': 'Swin Transformer (Swin-Tiny)',
            'text_encoder': 'BLIP (Salesforce/blip-image-captioning-base)',
            'fusion_method': 'MOE (Mixture of Experts, 6 experts)',
            'fusion_type': 'Dual-input (Visual + Text)',
        },
        'features': [
            'è§†è§‰ç‰¹å¾æå–ï¼ˆSwin Transformerï¼‰',
            'æ–‡æœ¬ç‰¹å¾æå–ï¼ˆBLIPï¼‰',
            'å¤šæ¨¡æ€ç‰¹å¾èåˆï¼ˆMOEåŒè¾“å…¥ï¼‰',
            'ç¾å­¦è¯„åˆ†é¢„æµ‹'
        ],
        'status': 'ready'
    }), 200


@app.route('/process_image', methods=['POST'])
def process_image():
    """
    å¤„ç†ä¸Šä¼ çš„å›¾åƒå¹¶è¿”å›å¤šæ¨¡æ€ç¾å­¦è¯„ä¼°ç»“æœ

    è¯·æ±‚:
        - image: å›¾åƒæ–‡ä»¶ï¼ˆmultipart/form-dataï¼‰

    è¿”å›:
        - aesthetic_score: ç¾å­¦è¯„åˆ†ï¼ˆåŸºäºè§†è§‰+æ–‡æœ¬ç‰¹å¾èåˆï¼‰
        - image_caption: BLIPç”Ÿæˆçš„å›¾åƒæè¿°
        - api_description: GPTç”Ÿæˆçš„è¯¦ç»†ç¾å­¦æè¿°
        - model_info: æ¨¡å‹ä¿¡æ¯
    """
    if 'image' not in request.files:
        return jsonify({'error': 'è¯·æ±‚ä¸­æ²¡æœ‰å›¾åƒæ–‡ä»¶'}), 400

    file = request.files['image']
    if file.filename == '':
        return jsonify({'error': 'æœªé€‰æ‹©å›¾åƒ'}), 400

    try:
        # æ‰“å¼€å¹¶é¢„å¤„ç†å›¾åƒ
        img = Image.open(file.stream).convert('RGB')
        image_tensor = transform(img).unsqueeze(0).to(device)

        with torch.no_grad():
            # âœ… ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹è·å–è¯„åˆ†
            # forward()å†…éƒ¨ä¼šè‡ªåŠ¨ï¼š
            # 1. æå–è§†è§‰ç‰¹å¾ï¼ˆSwinï¼‰
            # 2. æå–æ–‡æœ¬ç‰¹å¾ï¼ˆBLIPï¼‰
            # 3. åŒè¾“å…¥èåˆï¼ˆMOEï¼‰
            aesthetic_score_tensor = aesthetic_model(image_tensor)
            aesthetic_score = aesthetic_score_tensor.item()

            # å•ç‹¬ç”Ÿæˆå›¾åƒæè¿°ç”¨äºå±•ç¤º
            inputs_caption = aesthetic_model.blip_processor(images=img, return_tensors="pt").to(device)
            outputs_caption = aesthetic_model.blip_model.generate(**inputs_caption, max_length=50)
            image_caption = aesthetic_model.blip_processor.decode(outputs_caption[0], skip_special_tokens=True)

            # è°ƒç”¨APIç”Ÿæˆè¯¦ç»†æè¿°
            api_description = generate_description_api(aesthetic_score, image_caption, API_KEY)

        return jsonify({
            'aesthetic_score': f"{aesthetic_score:.2f}",
            'image_caption': image_caption,
            'api_description': api_description,
            'model_info': {
                'type': 'multimodal',
                'visual_encoder': 'Swin-Tiny',
                'text_encoder': 'BLIP',
                'fusion_method': 'MOE (6 experts)',
                'fusion_inputs': 'Visual features + Text features',
                'note': 'âœ… çœŸæ­£çš„å¤šæ¨¡æ€èåˆï¼ˆç¬¦åˆè®ºæ–‡ç†è®ºï¼‰'
            }
        }), 200

    except Exception as e:
        print(f"å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'å¤„ç†å›¾åƒæ—¶å‡ºé”™', 'details': str(e)}), 500


# ============= å¯åŠ¨æœåŠ¡ =============
if __name__ == '__main__':
    print("\n" + "="*70)
    print("ğŸ¨ å¤šæ¨¡æ€å›¾åƒç¾å­¦è¯„ä»·ç³»ç»Ÿ")
    print("="*70)
    print(f"ğŸ“± è®¾å¤‡: {device}")
    print(f"ğŸ¤– æ¨¡å‹: MultimodalAestheticModel")
    print(f"ğŸ‘ï¸  è§†è§‰ç¼–ç å™¨: Swin Transformer (Swin-Tiny)")
    print(f"ğŸ“ æ–‡æœ¬ç¼–ç å™¨: BLIP (Salesforce/blip-image-captioning-base)")
    print(f"ğŸ”€ èåˆæ–¹æ³•: MOE (6 experts, dual-input)")
    print(f"âœ… å¤šæ¨¡æ€èåˆ: è§†è§‰ç‰¹å¾ + æ–‡æœ¬ç‰¹å¾")
    print("="*70)
    print("ğŸ“¡ APIç«¯ç‚¹:")
    print("   GET  /test           - æµ‹è¯•æœåŠ¡çŠ¶æ€")
    print("   POST /process_image  - è¯„ä¼°å›¾åƒç¾å­¦")
    print("="*70)
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    print("="*70 + "\n")

    app.run(debug=True, port=5000, host='0.0.0.0')
